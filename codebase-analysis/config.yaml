# LLM Code Grapher Configuration

llm:
  provider: "openai"  # openai, anthropic, ollama
  model: "gpt-4"
  max_tokens: 4000
  temperature: 0.1
  api_key: null  # Set via environment variable
  base_url: null  # For Ollama and other local providers (e.g., http://localhost:11434)

processing:
  chunk_size: 2000
  overlap: 200
  max_retries: 3
  parallel_sections: true
  confidence_threshold: 0.7

dependency_filtering:
  enabled: true
  max_line_distance: 50
  max_operations_between: 10
  skip_common_params: true
  common_param_prefixes: ["ws-", "ws_", "working-storage", "file-", "file_", "input-", "output-", "temp-", "temp_"]

language:
  # Language-specific patterns will be loaded dynamically based on the language being analyzed
  # This section is populated at runtime by the language-specific modules
  section_patterns: []
  subsection_patterns: []
  data_patterns: []

output:
  format: "json"  # json, yaml, graphml
  include_confidence: true
  generate_visualization: true
  output_dir: "./output"
  filename_template: "{input_name}_analysis_{timestamp}"

logging:
  level: "INFO"
  file: "./logs/llm_code_grapher.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
